---
title: "Who Says Yes to Coupons? A Statistical Study of Drivers' Acceptance"
output: html_document
---

```{r echo=FALSE, include=FALSE}
library(tidyverse)
library(ggplot2)
library(tibble)
library(testequavar)
library(kableExtra)
```

#### By Thomas Linden, John Nobles, Phung Tran, Oumayma Ben Aoun

------------------------------------------------------------------------

### Introduction

The project goal is to predict whether a driver will accept a coupon recommended to them based of the type of coupon, demographic profile, and conditions such as weather, time, etc., with data from the UC Irvine Machine Learning Repository.

```{r echo=FALSE}
coupons <- read.csv('in-vehicle-coupon-rec.csv')
colnames(coupons)
```

Statistical methods used to compare data are, Bootstrapping, ANOVA (with a Tukey post-hoc test), logistic regression, and permutation testing. These test were chosen to test the relationship between driving contexts and the binary decision to accept or reject a coupon. Both parametric (ANOVA, Logistic Regression) and non parametric (Bootstrapping, Permutation) approaches were used. This is because the outcome between specific driving contexts are binary, with accept (Y = 1), or reject (Y = 0) a coupon.The large sample size allows us to rely on the Central Limit Theorem. For other questions with multiple categories, the F statistic from ANOVA indicates variation between the groups. Bootstrapping and Permutations testes were used to validate findings where assumptions were violated.

------------------------------------------------------------------------

### 1. Does expiration length affect acceptance?

$H_0:$ The mean acceptance is equal for $1$-day and $2$-hour coupons, i.e., $\mu_{2h} - \mu_{1d} = 0$.

$H_A:$ The mean acceptance differs between expiration lengths, i.e., $\mu_{2h} - \mu_{1d} \neq 0$.

#### Justification

Since observations are IID, there is exchangeability between groups, and the sample size is large, we will conduct a test for the difference in proportions between $1$-day and $2$-hour coupons using bootstrap under the null.

```{r}
# This will be the data variable used for questions 1 & 5
# Load data:
coupons <- read.csv('in-vehicle-coupon-rec.csv')

# Verify sample size is large:
dim(coupons)

# Conduct bootstrap:
two_hour <- coupons %>% filter(expiration == '2h') %>% select(Y) 
one_day <- coupons %>% filter(expiration == '1d') %>% select(Y)

dif_proportion_obs <- mean(two_hour$Y)-mean(one_day$Y)
dif_proportion_obs

B <- 10000
dif_proportion_boot <- numeric(B)
n_2h <- length(two_hour$Y)
n_1d <- length(one_day$Y)
pool <- c(two_hour$Y, one_day$Y)

for (i in 1:B) {
  boot_pool <- sample(pool, n_2h+n_1d, replace=TRUE)
  boot_2h <- sample(boot_pool, n_2h, replace=TRUE)
  boot_1d <- sample(boot_pool, n_1d, replace=TRUE)
  dif_proportion_boot[i] <- mean(boot_2h)-mean(boot_1d)
}

p_value <- mean(abs(dif_proportion_boot) >= abs(dif_proportion_obs))

# Display p-value:
p_value
```

#### Interpretation

The observed difference in acceptance proportions was $\mu_{2h} - \mu_{1d} = -0.1296$, and the bootstrap test under the null resulted in a very low p-value of $p < 0.0001$. With a significance level of $\alpha = 0.05$, this suggests that the true difference in acceptance proportions is not equal to zero, that is, expiration length does affect acceptance.

------------------------------------------------------------------------

### 2. Does time of day relate to acceptance?

Variables (Description): time (categorical: 5 levels — morning, afternoon, evening, late night, unknown); acceptance (binary)

Data Type: Multi-level categorical on binary outcome

Planned Method of Analysis: One-way ANOVA on mean acceptance rates by time of day

$H_0$: Mean acceptance is the same across all times of day.

$H_A$: At least one time-of-day group has a different mean acceptance.

```{r}
# This will be the data variable used for questions 2 & 4
# Load data 
data <- read.csv("in-vehicle-coupon-rec.csv")
head(data)
```

```{r}
# Time order
data$time <- factor(data$time, 
                    levels = c("7AM", "10AM", "2PM", "6PM", "10PM"),
                    ordered = TRUE)

# The table shows the % of acceptances for each time slot
(round(prop.table(table(data$time, data$Y), margin = 1), 3) * 100)[, "1"] 

# One-way ANOVA — exactly the method the assignment asked for
anova_result <- aov(Y ~ time, data = data) # (We treat Y as numeric 0/1, so the mean = acceptance rate)
summary(anova_result)# If p < 0.05 then yes, time of day is related to acceptance
```

#### Interpretation

The one-way ANOVA revealed a highly significant effect of time of day on coupon acceptance, F(4, 12679) = 44.37, p \< 2e-16. We therefore reject the null hypothesis and conclude that acceptance rates vary significantly depending on the time the coupon is offered. Acceptance was highest at 2 PM (66.2%), followed by 10 AM (60.8%) and 6 PM (58.5%), and clearly lower in the early morning at 7 AM (50.2%) and especially late at night at 10 PM (50.8%). Overall, customers are substantially more likely to accept coupons during the late morning, midday, and early evening hours than in the early morning or late night. In practical terms, coupon distribution should be concentrated between 10 AM and 6 PM, with peak effectiveness around 2 PM, and minimized during the 7 AM and 10 PM periods.

------------------------------------------------------------------------

### 3. Do travel time/direction costs matter?

$H_0$: Travel time and direction have no effect on mean acceptance.

$H_A$: Travel time or direction significantly affect acceptance rates.

#### Justification

Bootstrapping is used to estimate the difference in proportions between same vs. opposite direction and short vs. long time drive, by creating a 95% Confidence Interval.

Since the outcome variable Y is binary, the mean represents the proportion of acceptance without relying on assumptions of normality.

```{r}
# This will be the data variable used for questions 3 & 7
coupon_data <- read.csv("in-vehicle-coupon-rec.csv")
```

```{r}
# multiple bootstrap 
set.seed(123)
N <- 10^4

# Direction 

Y_same <- subset(coupon_data, select = Y, direction_same == 1, drop = TRUE) # same direction
Y_opp <- subset(coupon_data, select = Y, direction_same == 0, drop = TRUE) # opposite direction

# opposite direction
n_opp <- length(Y_opp)

# same direction
n_same <- length(Y_same)

boot_diffs_dir <- numeric(N)

# Time

Y_short <- subset(coupon_data, select = Y, toCoupon_GEQ25min == 0, drop = TRUE)
Y_long <- subset(coupon_data, select = Y, toCoupon_GEQ25min == 1, drop = TRUE)

# <25 min as short drive 
n_short <- length(Y_short)

# >25 min as long drive 
n_long <- length(Y_long)

boot_diffs_time <- numeric(N)

```

```{r}
# bootstrap for direction
for(i in 1:N)
{
  same_sample <- sample(Y_same, n_same, replace = TRUE)
  opp_sample <- sample(Y_opp, n_opp, replace = TRUE)
  
  boot_diffs_dir[i] <- mean(same_sample) - mean(opp_sample)
}  

quantile(boot_diffs_dir, c(0.025, 0.975))
```

```{r}
# bootstrap for time
for(i in 1:N)
{
 short_sample <- sample(Y_short, n_short, replace = TRUE)
 long_sample <- sample(Y_long, n_long, replace = TRUE)
 
 boot_diffs_time[i] <- mean(short_sample) - mean(long_sample)
}

quantile(boot_diffs_time, c(0.025, 0.975))
```

```{r}
par(mfrow = c(1,2))
hist(boot_diffs_dir, main = 'Boot Mean Diff. Dist. : Direction', xlab = 'Mean Difference')

hist(boot_diffs_time, main = 'Boot Mean Diff. Dist. : Time', xlab = 'Mean Difference')
```

#### Bootstrap Interpretation

**Time:** The 95% confidence interval, [0.131, 0.185], for time does not include 0, indicating that time is statistically significant.This can be seein in the histogram, distribution is centered at around 0.16, with 0 not included. The interval is positive, showing that the acceptance rate for short drives is higher than long drives.

**Direction:** The 95% confidence interval, [-0.003, 0.038], for direction includes 0, indicating that direction is not statistically significant.The distribution is centered at 0. making zero a plausible value.There is not enough information to say that direction alone causes people to accept or reject a coupon.

#### ANOVA Test

A Two-Way ANOVA test is typically for continuous numbers, in this data set, there is a binary response instead. ANOVA, in this case would let us know the acceptance rate changes based on two different factors, effect of time and direction, at the same time.

#### Questions:

**Time:** Does driving \>25 min change the decision to accept the coupon?

**Direction:** Does driving in the opposite direction change the decision to accept the coupon?

**Both:** Does the direction matter more when the drive is long?

```{r}
# two way ANNOVA

#factor to organize

coupon_data$time <- factor(coupon_data$toCoupon_GEQ25min, levels = c(0,1), labels = c('Short Time (<25)', 'Long Time (>25)'))

coupon_data$dir <- factor(coupon_data$direction_same, levels = c(1,0), labels = c('Same Direction', 'Opposite Direction'))

annova_model <- aov(Y ~ time * dir, data = coupon_data)

summary(annova_model) 
```

#### Interpretation

**Effect of Time:** At significance level 0.005, there is a statistical significance for time. The p value is less than 0.05 alpha, so we fail to accept the null hypothesis. Driving \>25 min does change the drivers decision to accept the coupon.

**Effect of Direction:** There is not a statistical significance for direction. The p value is greater than 0.005 alpha, so we fail to reject the null hypothesis. The data cannot confirm or deny that directions alone causes people to accept or reject the coupon, reconfirming the conclusion drawn from bootstrapping.

```{r}
# plot for ANOVA 

# sum of total
group_mean <- aggregate(Y ~ time + dir, data = coupon_data, FUN = mean)

ggplot(group_mean, aes(x = time, y = Y, group = dir, color = dir)) + geom_line(size = 1.2, position = position_dodge(width = 0.2)) + geom_point(size = 4, position = position_dodge(width = 0.2)) +
  labs(title = "Two-Way ANOVA",
       y = "Acceptance Rate (Mean Y)",
       x = "Driving Time") +
  theme_minimal()
```

#### Interpretation

There is no blue dot for on the left side representing Long Time (\> 25) min. That means there are no drivers who were even offered a coupon that has a long drive **and** in the same direction.

**Does time cost matter?**

Yes, time cost does matter, as the red line slopes downward. When the drive time is \>25 min, the acceptance rate for the coupon drops.

**Does Direction cost matter?**

Direction does not matter for drive times \<25 min. People accept the coupon at the same rate whether it's in the opposite direction or not.

**Does the direction matter more when the drive is long?**

There is no data on long drives in the same direction so this question cannot be answered.

```{r}
# contingency table

# average 0 to 1 for each group
mean_table <- tapply(coupon_data$Y,list(coupon_data$time, coupon_data$dir), mean)

# how many drivers in each group
count_table <- table(coupon_data$time, coupon_data$dir)

mean_table
count_table
```

#### Interpretation

The mean average contingency table shows that for short drives, direction didn't matter at the average was almost identical at 0.58. The table also reaffirms that no drivers were offered a coupon that required a long drive in the same direction.

------------------------------------------------------------------------

### 4. Is acceptance different by coupon type?

Variables (Description): coupon (categorical: 4 levels — bar, coffee house, \<\$20 restaurant, \$20–\$50 restaurant); acceptance (binary)

Data Type: Categorical predictor with four levels One-way ANOVA on acceptance means by coupon category

$H_0$: Mean acceptance is equal across all coupon types.

$H_A$: At least one coupon type has a different mean acceptance.

```{r}
# 'coupon' is treated as a categorical variable (factor)
data$coupon <- factor(data$coupon)

(round(100 * prop.table(table(data$coupon, data$Y), margin = 1), 1)[, "1"])# shows which coupon works best
  
# One-way ANOVA 
summary(aov(Y ~ coupon, data = data)) # treat Y as 0/1 numbers, so the group mean = acceptance rate
# Look at the p-value in the last column: if p < 0.05 ....> YES, acceptance differs by coupon type
```

The one-way ANOVA revealed a highly significant effect of coupon type on acceptance rate, F(4, 12679) = 234.5, p \< 2e-16. We therefore reject the null hypothesis and conclude that acceptance rates differ substantially across coupon types. Customers are far more likely to accept Carry out & Take away coupons (73.5%) and Restaurant(\<20) coupons (70.7%) than any other type. Acceptance drops noticeably for Coffee House coupons (49.9%) and is lowest for Restaurant(20–50) (44.1%) and Bar coupons (41.0%). In practical terms, inexpensive, quick, and convenient food offers are redeemed at dramatically higher rates, whereas coupons for mid-range restaurants and bars yield the poorest return. To maximize redemption rates, marketing efforts should prioritize Carry out & Take away and cheap restaurant (\<20) coupons and de-emphasize bar and 20–50 restaurant promotions.

------------------------------------------------------------------------

### 5. Do past visit frequencies predict accepting the matching coupon type?

$H_0:$ The visit frequency and coupon type have no effect on mean acceptance.

$H_A:$ The visit frequency or coupon type significantly affect acceptance.

#### Justification

We will perform a logistic regression. The assumptions for this method are met because we have a large sample size, independent observations, and a binary response variable.

#### Explanation

We first had to create a new column that matched the restaurant type of the coupon to the visit frequency of that restaurant type. Using the output of the logistic regression, we can analyze the p-values from the summary and the odds ratios from the coefficients output. The output is long, so we commented it out after making our analysis.

```{r}
# New column creation:
coupons <- coupons %>%
  mutate(
    visit_freq = case_when(
      coupon == "Coffee House" ~ CoffeeHouse,
      coupon == "Bar" ~ Bar,
      coupon == "Carry out & Take away" ~ CarryAway,
      coupon == "Restaurant(20-50)" ~ Restaurant20To50,
      coupon == "Restaurant(<20)" ~ RestaurantLessThan20))

unique(coupons$coupon)
model <- glm(Y ~ coupon * visit_freq, family = binomial, data = coupons)
# summary(model)
# exp(coef(model))
```

#### Interpretation

There were several instances where past visit frequencies had large effects. In fact, out of the $24$ comparisons displayed by the model summary, $15$ of them were significant at the $\alpha=0.05$ significance level. The visit frequency reference level was those who visited a given restaurant type one to three times per month, and the restaurant type reference level was Bars. That is to say, we'll be making all comparisons to a baseline group, those who visit bars $1$-$3$ times per month. We chose five of the most significant (lowest p-value) comparisons to interpret:

Note: For these interpretations, we used "cheap restaurant" for restaurants costing less than \$$20$ and "expensive restaurant" for restaurants costing between \$$20$ and \$$50$.

1.  In general, those who reported "never visit" had $87$% lower odds to accept the coupon compared to the baseline ($p\approx 0$). This makes sense since one would probably not accept a coupon to a restaurant they never go to anyways.

2.  Compared to the baseline, those who visited cheap restaurants less than once per month had $101$% higher odds to accept a coupon for a cheap restaurant ($p\approx 0.0001$). This seems to contradict the notion from our previous comparison; instead of people who go to this type of restaurant less having smaller odds to accept a coupon for them, the opposite happened and they were much more likely to accept. This could be due to the restaurant type. That is, maybe these cheaper restaurants, perhaps with lower quality food, are something that people would not spend their own money on, but they would be happy with getting a cheap or free meal with a coupon.

3.  Those who never order carry out had $753$% higher odds to accept a coupon than the baseline ($p \approx 0.000001$). This comparison seems to build on our previous one in that despite the group in question never ordering carry out, they were astronomically more likely to accept a coupon. We think this could be because they don't value carry out food enough to spend their own money on it (perhaps they think it's too unhealthy, or some other reason), but if given the opportunity for free or cheap food with a coupon, they wouldn't mind having a cheat day.

4.  Compared to the baseline, those who never visit cheap restaurants had $319$% higher odds to accept a coupon ($p \approx 0.000005$). The trend from our previous two comparisons seems to continue; "never visit-ors" happily accept coupons to somewhere they appear to not care much for.

5.  Once again, the trend continues. Even in expensive restaurants, those who never visit had $198$% higher odds of accepting such a coupon compared to the baseline ($p \approx 0.000001$).

------------------------------------------------------------------------

### 6. Does who’s in the car relate to acceptance?

```{r}
# Import coupon dataset and fix passenger name.
# This will be the data variable name used for questions 6 & 8.
dataset <- read.csv("in-vehicle-coupon-rec.csv")
dataset <- dataset %>% rename(passenger = passanger)
```

```{r echo=FALSE}
# Tabulate counts of accepted (Y = 1) vs not (Y = 0) within each passenger group.
table(dataset$passenger, dataset$Y)

# Compute acceptance proportion for each passenger group and order from lowest to highest.
q6.props <- dataset %>%
  group_by(passenger) %>%
  summarize(proportion = mean(Y), .groups = "drop") %>%
  arrange(proportion) %>%
  mutate(passenger = factor(passenger, levels = passenger))

# Plot acceptance proportion by passenger group (ordered by proportion).
ggplot(
  q6.props,
  aes(x = passenger, y = proportion)
) +
  geom_col() +
  labs(
    x = "Passenger group",
    y = "Proportion",
    title = "Coupon acceptance proportion by passenger group"
  ) +
  theme_minimal()
```

#### Explanation

Based on the group-level proportions, we can see the groups that are more likely to accept coupons, however, we need to check whether these differences are statistically significant or could be explained by random variation.

Before performing ANOVA, we check the equal-variance assumption using equavar4test from the testequavar package.

We test-

$H_0: \sigma_{Alone}^2 = \sigma_{Friend(s)}^2 =\sigma_{Kid(s)}^2 =\sigma_{Partner}^2$ (equal variances)

$H_A: \text{At least one group has a different variance.}$

```{r echo=FALSE}
# Extract the unique passenger group labels (e.g., Alone, Friend(s), Kid(s), Partner).
passenger.groups <- unique(dataset$passenger)

# Permutation test for equality of variances across the four passenger groups.
# a = 0.05 sets the significance level; B = 500 is the number of permutations.
equa4vartest(
  dataset$Y[dataset$passenger == passenger.groups[1]],
  dataset$Y[dataset$passenger == passenger.groups[2]],
  dataset$Y[dataset$passenger == passenger.groups[3]],
  dataset$Y[dataset$passenger == passenger.groups[4]],
  a = 0.05, B = 500
)
```

#### Interpretation

The permutation-based equal-variance test gave $p = 0.002$, providing strong evidence against equal variances. For binary outcomes, each group’s variance is $p(1-p)$, so when group means differ, their variances are not exactly equal and strict homogeneity of variance is unlikely. We include the equal-variance test for completeness, but we use ANOVA F-test and Tukey’s HSD as large-sample approximations: although the group sizes are unequal, each group still has a substantial sample size, so these methods are reasonably robust in practice. A more exact analysis could instead use logistic regression or Welch-type methods (with unequal-variance post-hoc comparisons), but the resulting conclusions are expected to be very similar.

We test-

$H_0:$ Mean acceptance is equal across all passenger groups.

$H_A:$ At least 1 passenger group has a different mean acceptance.

Method: One-way ANOVA on passenger and acceptance columns.

```{r echo=FALSE}
# Fit a one-way ANOVA model for acceptance (Y) by passenger group.
q6.lm <- lm(Y ~ as.factor(passenger), data = dataset)

# ANOVA table to test if mean acceptance differs across passenger groups.
anova(q6.lm)
```

The ANOVA F-test is highly significant (p \< 0.001), indicating that at least one passenger group has a different mean acceptance rate. We therefore follow up with Tukey’s HSD to identify which groups differ.

```{r echo=FALSE}
# Convert the linear model to an ANOVA object (required by TukeyHSD).
q6.fit <- aov(q6.lm)

# Tukey's HSD post-hoc comparisons for all passenger-group pairs.
q6.tk <- TukeyHSD(q6.fit)

# Display pairwise differences, confidence intervals, and adjusted p-values.
q6.tk
```

#### Interpretation

Since acceptance within each group is modeled as the proportion of 1’s (coupon accepted), the group mean is the estimated probability of acceptance. The Tukey output therefore reports differences in proportions (i.e., differences in average acceptance probabilities), and the lwr and upr columns give 95% confidence intervals for those differences. Positive differences mean the first group in the contrast label has a higher acceptance probability. All pairwise Tukey (studentized range) comparisons are significant at the 0.001 level except Kid(s) − Alone, so given these data we do not have evidence of a difference in acceptance rates between those driving alone and those with kid(s).

Using Tukey's method to control family-wise error at 5%, we are 95% confident, and estimate that, on average, a person is:

-   12.11% to 17.41% **more likely** to accept a coupon if they are with friend(s) than when they are alone.
-   2.83% to 11.08% **more likely** to accept a coupon if they are with their partner than when they are alone.
-   12.30% to 21.39% **less likely** to accept a coupon if they are with kid(s) than when they are with friend(s).
-   3.38% to 12.24% **less likely** to accept a coupon if they are with their partner than when they are with friend(s).
-   3.50% to 14.57% **more likely** to accept a coupon if they are with their partner than when they are with kid(s)

```{r echo=FALSE}
# Rebuild data from the model so we do not depend on the original dataset object.
dat <- model.frame(q6.lm)
colnames(dat) <- c("accept", "passenger")

# Compute group-level acceptance rates.
group_props <- dat %>%
  group_by(passenger) %>%
  summarise(prop_acc = mean(accept == 1), .groups = "drop")

# Extract Tukey results for passenger comparisons.
tk <- TukeyHSD(aov(q6.lm), "as.factor(passenger)")

tk_df <- as.data.frame(tk$`as.factor(passenger)`) %>%
  rownames_to_column("contrast") %>%
  separate(contrast, into = c("group2", "group1"), sep = "-") %>%
  left_join(group_props, by = c("group1" = "passenger")) %>%
  rename(prop1 = prop_acc) %>%
  left_join(group_props, by = c("group2" = "passenger")) %>%
  rename(prop2 = prop_acc) %>%
  mutate(
    lower_group  = if_else(prop1 < prop2, group1, group2),
    higher_group = if_else(prop1 < prop2, group2, group1),
    label        = paste(group2, "-", group1)
  )

# Build CI segments for the less-likely (left) and more-likely (right) groups.
seg_left <- tk_df %>%
  transmute(
    label,
    y_start   = lwr,
    y_end     = diff,
    side_group = lower_group
  )

seg_right <- tk_df %>%
  transmute(
    label,
    y_start   = diff,
    y_end     = upr,
    side_group = higher_group
  )

segments <- bind_rows(seg_left, seg_right)

# Color palette for passenger groups.
passenger_cols <- c(
  "Alone"     = "#1b9e77",
  "Friend(s)" = "#d95f02",
  "Kid(s)"    = "#7570b3",
  "Partner"   = "#e7298a"
)

ggplot() +
  geom_hline(yintercept = 0, linetype = "dashed") +
  geom_segment(
    data = segments,
    aes(
      x    = label,
      xend = label,
      y    = y_start,
      yend = y_end,
      color = side_group
    ),
    linewidth = 1.2
  ) +
  geom_point(
    data = tk_df,
    aes(x = label, y = diff),
    color = "black",
    size = 2.2
  ) +
  coord_flip() +
  scale_color_manual(values = passenger_cols, name = "Group") +
  labs(
    x = "Passenger group comparison",
    y = "Difference in acceptance proportion",
    title = "Pairwise differences in coupon acceptance (Tukey 95% CIs)"
  ) +
  theme_minimal()
```

#### Explanation

The image above shows Tukey 95% confidence intervals for the difference in coupon‐acceptance proportions for each pair of passenger groups. Each point is the estimated difference in acceptance probability, and the horizontal line is the corresponding 95% confidence interval. The dashed vertical line at 0 represents “no difference”: intervals entirely to the right of 0 indicate that the first group in the label (e.g., Friend(s) – Alone) has a higher acceptance rate; intervals entirely to the left of 0 indicate a lower acceptance rate. For each comparison, the CI segment is colored by the group at that side: the left segment is colored by the less-likely group, and the right segment by the more-likely group.

------------------------------------------------------------------------

### 7. Are there weather effects?

$H_0$: Weather and temperature have no effect on acceptance (no main or interaction effects).

$H_A$: Weather or temperature significantly affect acceptance.

```{r}
# two way ANNOVA

coupon_data$temp_cat <- factor(coupon_data$temperature, levels = c(30,55,80), labels = c('Low (30F)', 'Med (55F)', 'High (80F)'))

coupon_data$weather_cat <- factor(coupon_data$weather, levels = c('Snowy', 'Rainy', 'Sunny'))

weather_anova <- aov(Y ~ weather_cat * temp_cat, data = coupon_data)
summary(weather_anova)
```

#### Interpretation

**Weather:** At significant level 0.005, weather has a p value considerably less then 0.05. The null hypothesis is rejected in this case, indicating that weather does have an affect on coupon acceptance.

**Temp:** At significance level 0.005, there is a statistical significance for time. The null hypothesis is rejected in this case, indicating that temp does have an affect on coupon acceptance.

```{r}
# contingency table

mean_weather <- tapply(coupon_data$Y, list(coupon_data$weather_cat, coupon_data$temp_cat), mean)

count_weather <- table(coupon_data$weather_cat)

mean_weather
count_weather
```

#### Interpretation

**Effect of Snow:** Comparison of Sunny and Snowy at 30F shows that, when it snows, people are less likely to accept a coupon

**Effect of Rain:** Comparison of Sunny and Rainy at 55F shows that, people are less likely to accept a coupon.

**Effect of Temp:** When it is Sunny, People are more likely to accept a coupon when it's 30F (62%), then 80F (60%), with 55F (57%), having the lowest acceptance rate of the three.

The contingency table also shows that adverse weather conditions significantly reduce acceptance rates. On sunny days, acceptance rates are similar across all 3 temp. Bad weather is a stonger deterant than temperature itself.

```{r}
group_means_w <- aggregate(Y ~ weather_cat + temp_cat, data = coupon_data, FUN = mean)

ggplot(group_means_w, aes(x = temp_cat, y = Y, group = weather_cat, color = weather_cat)) + 
  geom_line(size = 1.2, position = position_dodge(width = 0.1)) + 
  geom_point(size = 4, position = position_dodge(width = 0.1)) + 
  labs(title = "Interaction Plot: Weather vs. Temperature",
       y = "Acceptance Rate",
       x = "Temperature",
       color = "Weather") +
  theme_minimal()
```

#### Interpretation

The above plot gives a view on the results of the contingency table. The red dot indicating snow has no line because it only snows at 30F. The green dot indicating rain is also isolated because the data only captures rain at 55F. The blue line indicating sun, is the only like because sunny weather can happen at all 3 temperatures.

There are large gaps between each dot varivable for temp. Snow makes people reject coupons. Rain is the most significant weather with higher coupon rejection rates. When it's sunny, changes in temp alone has minimal effects on wheater a driver will accept or reject a coupon.

------------------------------------------------------------------------

### 8. Do age groups or income bands relate to coupon acceptance?

```{r}
# Define the desired ordering for age and income factor levels.
age_levels <- c("below21", "21", "26", "31", "36", "41", "46", "50plus")
income_levels <- c(
  "Less than $12500",
  "$12500 - $24999",
  "$25000 - $37499",
  "$37500 - $49999",
  "$50000 - $62499",
  "$62500 - $74999",
  "$75000 - $87499",
  "$87500 - $99999",
  "$100000 or More"
)

# Convert age and income columns to ordered factors with the specified levels.
dataset <- dataset %>%
  mutate(
    age    = factor(age,    levels = age_levels),
    income = factor(income, levels = income_levels)
  )

# --- Counts table: rows = income, columns = age ---

age_income_counts <- dataset %>%
  # Count the number of observations in each Income × Age combination.
  count(income, age, name = "n") %>%
  # Reshape to wide format so each age becomes a column.
  pivot_wider(
    names_from  = age,
    values_from = n,
    values_fill = 0         # Fill empty combinations with 0.
  )

# Display the table of sample sizes by income and age.
kable(
  age_income_counts,
  caption = "Counts by Income (rows) and Age (columns)"
)
```

#### Explanation

The table above shows the joint sample sizes across Age (columns) and Income (rows). Most Age × Income cells have several hundred observations, with only one empty cell (\$75k–\$87,499 & below21). So the design is somewhat unbalanced but not sparse, and a two-way logistic regression (Age, Income, and their interaction) is reasonable for this binary outcome.

Given these sample sizes, the next appropriate part of the analysis is to examine acceptance rates within each Age × Income group.

```{r}
# --- Proportion table: acceptance rate per Age × Income cell ---

age_income_props <- dataset %>%
  # Compute mean acceptance (proportion of Y = 1) for each Income × Age cell.
  group_by(income, age) %>%
  summarise(prop_Y = mean(Y), .groups = "drop") %>%
  # Reshape to wide format so each age group becomes a column.
  pivot_wider(
    names_from  = age,
    values_from = prop_Y,
    values_fill = NA      # Use NA where a cell has no observations.
  )

# Display the table of acceptance proportions by income and age.
kable(
  age_income_props,
  digits  = 3,
  caption = "Proportion accepted (Y) by Income (rows) and Age (columns)"
)

# --- Heatmap of observed acceptance proportions ---

age_income_long <- dataset %>%
  group_by(income, age) %>%
  summarise(prop_Y = mean(Y), .groups = "drop")

ggplot(age_income_long,
       aes(x = age, y = income, fill = prop_Y)) +
  geom_tile() +
  scale_fill_gradientn(
    name   = "Proportion\naccepted",
    limits = c(0, 1),
    colours = c(
      "#313695", "#4575b4", "#74add1", "#abd9e9",
      "#ffffbf", "#fdae61", "#f46d43", "#d73027", "#a50026"
    )
  ) +
  labs(
    x = "Age group",
    y = "Income band",
    title = "Coupon acceptance proportion by Age and Income"
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1)
  )
```

#### Justification

The proportion table and heatmap summarize the observed acceptance rate in each Age × Income cell. Acceptance varies from roughly 0.3 to 0.8, with generally higher rates for younger drivers (below21, 21, 26) and some mid- to lower-income groups, but the pattern is not monotonic in either age or income. These descriptive patterns suggest that both variables may relate to acceptance, but formal testing is needed because the groups differ in size and some cells are relatively small.

Logistic regression is appropriate here because the response is binary (0/1), and the model does not require equal variances or normality assumptions.

```{r}
# --- Logistic regression with Age × Income interaction ---

# Fit a logistic regression model for coupon acceptance (Y) with
# main effects of age and income plus their interaction.
model_full <- glm(
  Y ~ age * income,
  data   = dataset,
  family = binomial
)

# Likelihood-ratio (Chi-square) tests for age, income, and their interaction.
anova(model_full, test = "Chisq")
```

We fit a logistic regression model predicting coupon acceptance from Age, Income, and their interaction.

$\text{logit}(p) = \beta_0 + \beta_{\text{Age}_i} + \beta_{\text{Income}_j} + \beta_{(\text{Age}_i \space : \space \text{Income}_j)}$

Because the model includes a full interaction, each coefficient represents how acceptance differs from the reference group (below21 and “Less than \$12,500”) and how age and income effects change depending on one another.

#### Interpretation

The Analysis of Deviance shows that Age (χ² = 63.1, p \< 0.0001), Income (χ² = 47.9, p \< 0.0001), and especially the Age × Income interaction (χ² = 187.0, p \< 0.0001) all significantly improve model fit. This indicates that neither age nor income acts independently—the effect of income depends on age, and the effect of age depends on income.

One interaction coefficient was not estimable due to an empty Age × Income cell, which is consistent with the count table and does not affect the rest of the model.

Overall, the interaction model fits significantly better than the null model, although acceptance is clearly influenced by additional variables not included here. The next step is to visualize model-based predicted probabilities for each Age × Income combination.

```{r}
# ---- Model-based predictions using predict() ----

# Build grid of all Age x Income combinations.
new_grid <- expand.grid(
  age    = levels(dataset$age),
  income = levels(dataset$income)
)

# Predict on the logit scale with standard errors.
pred_link <- predict(
  model_full,
  newdata = new_grid,
  type   = "link",
  se.fit = TRUE
)

# Transform to predicted probabilities and 95% CIs.
new_grid <- new_grid %>%
  mutate(
    fit_link = pred_link$fit,
    se_link  = pred_link$se.fit,
    prob     = plogis(fit_link),                         # predicted probability
    lower    = plogis(fit_link - 1.96 * se_link),        # 95% CI lower
    upper    = plogis(fit_link + 1.96 * se_link)         # 95% CI upper
  )

# ---- Heatmap of predicted acceptance probabilities ----

ggplot(new_grid,
       aes(x = age, y = income, fill = prob)) +
  geom_tile() +
  scale_fill_gradientn(
    name   = "Predicted\nprobability",
    limits = c(0, 1),
    colours = c(
      "#313695", "#4575b4", "#74add1", "#abd9e9",
      "#ffffbf", "#fdae61", "#f46d43", "#d73027", "#a50026"
    )
  ) +
  labs(
    x = "Age group",
    y = "Income band",
    title = "Model-based coupon acceptance probability by Age and Income"
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1)
  )
```

#### Explanation

Using the fitted logistic model, we used predict() to compute model-based acceptance probabilities for every Age × Income combination. The predicted-probability heatmap smooths out sampling noise and shows the same qualitative pattern as the raw proportions: higher acceptance for many younger age groups, lower acceptance for some older/high-income combinations, and no simple increasing or decreasing trend in either variable alone. This visualization reinforces the conclusion that age and income jointly relate to coupon acceptance through a non-additive interaction.

### Conclusion

We employed bootstrapping and other statistical methods to address insightful questions related to our chosen coupon data. As a result of our analysis, we discovered multiple significant relationships in our data, including how coupon type, restaurant frequency, time of day, and types of passengers in the car affect acceptance rate. Many of these associations were logical and unsurprising; however, we found some that seemed to defy expectations, ultimately demonstrating the power and importance of statistical inference for uncovering relationships that wouldn’t have been noticed otherwise.

### References

Aché, M. (2020). in-vehicle coupon recommendation [Data set]. Kaggle. Retrieved December 5, 2025, from <https://www.kaggle.com/datasets/mathurinache/invehicle-coupon-recommendation>

Wang, T., Rudin, C., Doshi-Velez, F., Liu, Y., Klampfl, E., & MacNeille, P. (2017). A Bayesian framework for learning rule sets for interpretable classification. The Journal of Machine Learning Research, 18(1), 2357–2393.
